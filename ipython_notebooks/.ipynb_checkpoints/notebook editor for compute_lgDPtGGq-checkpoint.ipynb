{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import dataiku\n",
    "from dataiku import pandasutils as pdu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!home/ec2-user/miniconda3/bin/python3.7\n",
    "import os\n",
    "import sys\n",
    "import spacy\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "import json\n",
    "import requests\n",
    "import csv\n",
    "\n",
    "import urllib.request\n",
    "import gzip\n",
    "import sys\n",
    "import boto3\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_PATH = os.getcwd()\n",
    "sys.path.insert(0,BASE_PATH)\n",
    "NUM_CLUSTERS = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import aspect_extraction\n",
    "# import aspect_clustering\n",
    "# import mapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[\"i\",\"me\",\"my\",\"myself\",\"we\",\"our\",\"ours\",\"ourselves\",\"you\",\"you\\'re\",\"you\\'ve\",\"you\\'ll\",\"you\\'d\",\"your\",\"yours\",\"yourself\",\"yourselves\",\"he\",\"him\",\"his\",\"himself\",\"she\",\"she\\'s\",\"her\",\"hers\",\"herself\",\"it\",\"it\\'s\",\"its\",\"itself\",\"they\",\"them\",\"their\",\"theirs\",\"themselves\",\"what\",\"which\",\"who\",\"whom\",\"this\",\"that\",\"that\\'ll\",\"these\",\"those\",\"am\",\"is\",\"are\",\"was\",\"were\",\"be\",\"been\",\"being\",\"have\",\"has\",\"had\",\"having\",\"do\",\"does\",\"did\",\"doing\",\"a\",\"an\",\"the\",\"and\",\"but\",\"if\",\"or\",\"because\",\"as\",\"until\",\"while\",\"of\",\"at\",\"by\",\"for\",\"with\",\"about\",\"against\",\"between\",\"into\",\"through\",\"during\",\"before\",\"after\",\"above\",\"below\",\"to\",\"from\",\"up\",\"down\",\"in\",\"out\",\"on\",\"off\",\"over\",\"under\",\"again\",\"further\",\"then\",\"once\",\"here\",\"there\",\"when\",\"where\",\"why\",\"how\",\"all\",\"any\",\"both\",\"each\",\"few\",\"more\",\"most\",\"other\",\"some\",\"such\",\"no\",\"nor\",\"not\",\"only\",\"own\",\"same\",\"so\",\"than\",\"too\",\"very\",\"s\",\"t\",\"can\",\"will\",\"just\",\"don\",\"don\\'t\",\"should\",\"should\\'ve\",\"now\",\"d\",\"ll\",\"m\",\"o\",\"re\",\"ve\",\"y\",\"ain\",\"aren\",\"aren\\'t\",\"couldn\",\"couldn\\'t\",\"didn\",\"didn\\'t\",\"doesn\",\"doesn\\'t\",\"hadn\",\"hadn\\'t\",\"hasn\",\"hasn\\'t\",\"haven\",\"haven\\'t\",\"isn\",\"isn\\'t\",\"ma\",\"mightn\",\"mightn\\'t\",\"mustn\",\"mustn\\'t\",\"needn\",\"needn\\'t\",\"shan\",\"shan\\'t\",\"shouldn\",\"shouldn\\'t\",\"wasn\",\"wasn\\'t\",\"weren\",\"weren\\'t\",\"won\",\"won\\'t\",\"wouldn\",\"wouldn\\'t\"]'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataiku.get_custom_variables()['stopwords']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = [\"i\", \"me\", \"my\", \"myself\", \"we\", \"our\", \"ours\", \"ourselves\", \n",
    "             \"you\", \"you're\", \"you've\", \"you'll\", \"you'd\", \"your\", \"yours\", \"yourself\", \"yourselves\", \n",
    "             \"he\", \"him\", \"his\", \"himself\", \"she\", \"she's\", \"her\", \"hers\", \"herself\", \n",
    "             \"it\", \"it's\", \"its\", \"itself\", \"they\", \"them\", \"their\", \"theirs\", \"themselves\", \n",
    "             \"what\", \"which\", \"who\", \"whom\", \"this\", \"that\", \"that'll\", \"these\", \"those\", \n",
    "             \"am\", \"is\", \"are\", \"was\", \"were\", \"be\", \"been\", \"being\", \"have\", \"has\", \"had\", \"having\", \n",
    "             \"do\", \"does\", \"did\", \"doing\", \"a\", \"an\", \"the\", \"and\", \"but\", \"if\", \"or\", \n",
    "             \"because\", \"as\", \"until\", \"while\", \"of\", \"at\", \"by\", \"for\", \"with\", \"about\", \"against\", \n",
    "             \"between\", \"into\", \"through\", \"during\", \"before\", \"after\", \"above\", \"below\", \"to\", \"from\", \n",
    "             \"up\", \"down\", \"in\", \"out\", \"on\", \"off\", \"over\", \"under\", \"again\", \"further\", \"then\", \"once\", \n",
    "             \"here\", \"there\", \"when\", \"where\", \"why\", \"how\", \"all\", \"any\", \"both\", \"each\", \"few\", \"more\", \"most\", \n",
    "             \"other\", \"some\", \"such\", \"no\", \"nor\", \"not\", \"only\", \"own\", \"same\", \"so\", \"than\", \"too\", \"very\", \n",
    "             \"s\", \"t\", \"can\", \"will\", \"just\", \"don\", \"don't\", \"should\", \"should've\", \"now\", \"d\", \"ll\", \"m\", \"o\", \n",
    "             \"re\", \"ve\", \"y\", \"ain\", \"aren\", \"aren't\", \"couldn\", \"couldn't\", \"didn\", \"didn't\", \"doesn\", \"doesn't\", \n",
    "             \"hadn\", \"hadn't\", \"hasn\", \"hasn't\", \"haven\", \"haven't\", \"isn\", \"isn't\", \"ma\", \"mightn\", \"mightn\"t\", \n",
    "             \"mustn\", \"mustn't\", \"needn\", \"needn't\", \"shan\", \"shan't\", \"shouldn\", \"shouldn't\", \"wasn\", \"wasn't\", \n",
    "             \"weren\", \"weren't\", \"won\", \"won't\", \"wouldn\", \"wouldn't\"]\n",
    "\n",
    "exclude_stopwords = [\"it\",\"this\",\"they\",\"these\"] \n",
    "# update in aspect_extraction script as well to be replaced by \"product\"\n",
    "# stopwords = np.setdiff1d(stopwords, [\"it\",\"this\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_spacy(model_path):\n",
    "    print(\"\\nLoading spaCy Model....\")\n",
    "    nlp = spacy.load(model_path)\n",
    "    print(\"spaCy successfully loaded\")\n",
    "    for w in stopwords: # for the words in stopwords, set \"is_stop == True\" in spacy model\n",
    "        nlp.vocab[w].is_stop = True\n",
    "    for w in exclude_stopwords: # for the words in exclude_stopwords, set \"is_stop == False\" in spacy model\n",
    "        nlp.vocab[w].is_stop = False\n",
    "    return nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_nltk():\n",
    "    print(\"\\nLoading NLTK....\")\n",
    "    try :\n",
    "        sid = SentimentIntensityAnalyzer()\n",
    "    except LookupError:\n",
    "        print(\"Please install SentimentAnalyzer using : nltk.download('vader_lexicon')\")\n",
    "    print(\"NLTK successfully loaded\")\n",
    "    return(sid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(arg, text_column, review_id, product_id, csvname=None, tsvname=None):\n",
    "    model_path='/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/en_core_web_lg/en_core_web_lg-2.2.5'\n",
    "    time1 = time()\n",
    "    nlp = init_spacy(model_path)\n",
    "    sid = init_nltk()\n",
    "    time2 = time()\n",
    "    print(\"----------------***----------------\")\n",
    "    print(\"\\nExtracting aspect pairs\")\n",
    "    aspect_extraction.aspect_extraction(nlp, sid, arg, csvname, tsvname, \n",
    "                                        text_column = text_column, \n",
    "                                        review_id = review_id, \n",
    "                                        product_id = product_id)\n",
    "    print(\"Finished running aspect extraction!!\\n\")\n",
    "\n",
    "    # json_data = json.dumps(reviews_data)\n",
    "    # with open('data.json', 'w') as outfile:\n",
    "    #     f.write(json_data)\n",
    "\n",
    "    # print(\"----------------***----------------\")\n",
    "    # time3 = time()\n",
    "    # aspect_clustering.update_reviews_data(reviews_data, nlp)\n",
    "    time4 = time()\n",
    "    print(\"Time for spacy loading: {0:.2}s\".format(time2-time1))\n",
    "    # print(\"Time for aspect extraction: {0:.2}s\".format(time3-time2))\n",
    "    print(\"Time for EVERYTHING: {0:.2}s\".format(time4-time1))\n",
    "    print(\"Running mapper\")\n",
    "    file_in = \"data/interim/reviews_aspect_raw.json\"\n",
    "    file_out = \"data/processed/reviews_aspect_mapping.json\"\n",
    "    mapper.map(file_in, file_out)\n",
    "\n",
    "    print(\"Godspeed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You provided -f as the argument\n",
      "\n",
      "Loading spaCy Model....\n",
      "spaCy successfully loaded\n",
      "\n",
      "Loading NLTK....\n",
      "NLTK successfully loaded\n",
      "----------------***----------------\n",
      "\n",
      "Extracting aspect pairs\n",
      "&&&&& Running for the toy file &&&&&\n",
      "******Cleaning Started*****\n",
      "Shape of df before cleaning : (30790, 22)\n",
      "Shape of df after cleaning : (30790, 22)\n",
      "******Cleaning Ended*****\n",
      "Entering Apply function!\n",
      "Finished running aspect extraction!!\n",
      "\n",
      "Time for spacy loading: 1e+01s\n",
      "Time for EVERYTHING: 3e+02s\n",
      "Running mapper\n",
      "6\n",
      "30790\n",
      "Time for loads json: 0.15s\n",
      "Time for loads mapping: 0.025s\n",
      "Time for loads mapping: 0.6s\n",
      "Godspeed!\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__' :\n",
    "    print(f'You provided {sys.argv[1]} as the argument')\n",
    "\n",
    "    main(sys.argv[1], text_column = \"text\", review_id = 'tweet_id', product_id = 'company', \n",
    "         csvname='tweets_final.csv', tsvname=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read recipe inputs\n",
    "tweets_noURL = dataiku.Dataset(\"tweets_noURL\")\n",
    "tweets_noURL_df = tweets_noURL.get_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write recipe outputs\n",
    "aspect_sentiment_pairs = dataiku.Folder(\"lgDPtGGq\")\n",
    "aspect_sentiment_pairs_info = aspect_sentiment_pairs.get_info()"
   ]
  }
 ],
 "metadata": {
  "associatedRecipe": "compute_lgDPtGGq",
  "creator": "admin",
  "customFields": {},
  "kernelspec": {
   "display_name": "Python (env python36)",
   "language": "python",
   "name": "py-dku-venv-python36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "tags": [
   "recipe-editor"
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
