{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import dataiku\n",
    "import pandas as pd, numpy as np\n",
    "from dataiku import pandasutils as pdu\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from aspect_clustering.vector_dist import vector_dist\n",
    "from run_extraction.init_spacy import init_spacy\n",
    "# from aspect_clustering.add_clusters_to_reviews import add_clusters_to_reviews\n",
    "from sklearn import cluster\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read recipe inputs\n",
    "aspect_sentiment_pairs = dataiku.Dataset(\"tweepy_aspect_sentiment_pairs\")\n",
    "df = aspect_sentiment_pairs.get_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <button style=\"display:none\" \n",
       "            class=\"btn btn-default ipython-export-btn\" \n",
       "            id=\"btn-df-4c6d371e-29a7-4a2f-9bff-ea092d02daf6\" \n",
       "            onclick=\"_export_df('4c6d371e-29a7-4a2f-9bff-ea092d02daf6')\">\n",
       "                Export dataframe\n",
       "            </button>\n",
       "            \n",
       "            <script>\n",
       "                \n",
       "                function _check_export_df_possible(dfid,yes_fn,no_fn) {\n",
       "                    console.log('Checking dataframe exportability...')\n",
       "                    if(!IPython || !IPython.notebook || !IPython.notebook.kernel || !IPython.notebook.kernel) {\n",
       "                        console.log('Export is not possible (IPython kernel is not available)')\n",
       "                        if(no_fn) {\n",
       "                            no_fn();\n",
       "                        }\n",
       "                    } else {\n",
       "                        var pythonCode = 'from dataiku.notebook.export import IPythonExporter;IPythonExporter._check_export_stdout(\"'+dfid+'\")';\n",
       "                        IPython.notebook.kernel.execute(pythonCode,{iopub: {output: function(resp) {\n",
       "                            console.info(\"Exportability response\", resp);\n",
       "                            var size = /^([0-9]+)x([0-9]+)$/.exec(resp.content.data || resp.content.text)\n",
       "                            if(!size) {\n",
       "                                console.log('Export is not possible (dataframe is not in-memory anymore)')\n",
       "                                if(no_fn) {\n",
       "                                    no_fn();\n",
       "                                }\n",
       "                            } else {\n",
       "                                console.log('Export is possible')\n",
       "                                if(yes_fn) {\n",
       "                                    yes_fn(1*size[1],1*size[2]);\n",
       "                                }\n",
       "                            }\n",
       "                        }}});\n",
       "                    }\n",
       "                }\n",
       "            \n",
       "                function _export_df(dfid) {\n",
       "                    \n",
       "                    var btn = $('#btn-df-'+dfid);\n",
       "                    var btns = $('.ipython-export-btn');\n",
       "                    \n",
       "                    _check_export_df_possible(dfid,function() {\n",
       "                        \n",
       "                        window.parent.openExportModalFromIPython('Pandas dataframe',function(data) {\n",
       "                            btns.prop('disabled',true);\n",
       "                            btn.text('Exporting...');\n",
       "                            var command = 'from dataiku.notebook.export import IPythonExporter;IPythonExporter._run_export(\"'+dfid+'\",\"'+data.exportId+'\")';\n",
       "                            var callback = {iopub:{output: function(resp) {\n",
       "                                console.info(\"CB resp:\", resp);\n",
       "                                _check_export_df_possible(dfid,function(rows, cols) {\n",
       "                                    $('#btn-df-'+dfid)\n",
       "                                        .css('display','inline-block')\n",
       "                                        .text('Export this dataframe ('+rows+' rows, '+cols+' cols)')\n",
       "                                        .prop('disabled',false);\n",
       "                                },function() {\n",
       "                                    $('#btn-df-'+dfid).css('display','none');\n",
       "                                });\n",
       "                            }}};\n",
       "                            IPython.notebook.kernel.execute(command,callback,{silent:false}); // yes, silent now defaults to true. figures.\n",
       "                        });\n",
       "                    \n",
       "                    }, function(){\n",
       "                            alert('Unable to export : the Dataframe object is not loaded in memory');\n",
       "                            btn.css('display','none');\n",
       "                    });\n",
       "                    \n",
       "                }\n",
       "                \n",
       "                (function(dfid) {\n",
       "                \n",
       "                    var retryCount = 10;\n",
       "                \n",
       "                    function is_valid_websock(s) {\n",
       "                        return s && s.readyState==1;\n",
       "                    }\n",
       "                \n",
       "                    function check_conn() {\n",
       "                        \n",
       "                        if(!IPython || !IPython.notebook) {\n",
       "                            // Don't even try to go further\n",
       "                            return;\n",
       "                        }\n",
       "                        \n",
       "                        // Check if IPython is ready\n",
       "                        console.info(\"Checking conn ...\")\n",
       "                        if(IPython.notebook.kernel\n",
       "                        && IPython.notebook.kernel\n",
       "                        && is_valid_websock(IPython.notebook.kernel.ws)\n",
       "                        ) {\n",
       "                            \n",
       "                            _check_export_df_possible(dfid,function(rows, cols) {\n",
       "                                $('#btn-df-'+dfid).css('display','inline-block');\n",
       "                                $('#btn-df-'+dfid).text('Export this dataframe ('+rows+' rows, '+cols+' cols)');\n",
       "                            });\n",
       "                            \n",
       "                        } else {\n",
       "                            console.info(\"Conditions are not ok\", IPython.notebook.kernel);\n",
       "                            \n",
       "                            // Retry later\n",
       "                            \n",
       "                            if(retryCount>0) {\n",
       "                                setTimeout(check_conn,500);\n",
       "                                retryCount--;\n",
       "                            }\n",
       "                            \n",
       "                        }\n",
       "                    };\n",
       "                    \n",
       "                    setTimeout(check_conn,100);\n",
       "                    \n",
       "                })(\"4c6d371e-29a7-4a2f-9bff-ea092d02daf6\");\n",
       "                \n",
       "            </script>\n",
       "            \n",
       "        <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_id</th>\n",
       "      <th>review_id</th>\n",
       "      <th>noun</th>\n",
       "      <th>adj</th>\n",
       "      <th>rule</th>\n",
       "      <th>polarity_nltk</th>\n",
       "      <th>polarity_textblob</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Air France</td>\n",
       "      <td>1257399326906687489</td>\n",
       "      <td>flights</td>\n",
       "      <td>domestic</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Air France</td>\n",
       "      <td>1257397306170712064</td>\n",
       "      <td>bailout</td>\n",
       "      <td>french</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Air France</td>\n",
       "      <td>1257397306170712064</td>\n",
       "      <td>haul</td>\n",
       "      <td>short</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Air France</td>\n",
       "      <td>1257397306170712064</td>\n",
       "      <td>flights</td>\n",
       "      <td>domestic</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Air France</td>\n",
       "      <td>1257397306170712064</td>\n",
       "      <td>bailout</td>\n",
       "      <td>conditional</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   product_id            review_id     noun          adj  rule  polarity_nltk  polarity_textblob\n",
       "0  Air France  1257399326906687489  flights     domestic     1            0.0                0.0\n",
       "1  Air France  1257397306170712064  bailout       french     1            0.0                0.0\n",
       "2  Air France  1257397306170712064     haul        short     1            0.0                0.0\n",
       "3  Air France  1257397306170712064  flights     domestic     1            0.0                0.0\n",
       "4  Air France  1257397306170712064  bailout  conditional     3            0.0                0.0"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recategorize nouns == company names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "company_list = dataiku.get_custom_variables(typed=True)['company']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "company_list_lower = [x.lower() for x in company_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'air france': 'Air France',\n",
       " 'lufthansa': 'Lufthansa',\n",
       " 'ryanair': 'Ryanair',\n",
       " 'easyjet': 'easyJet',\n",
       " 'united airlines': 'United Airlines',\n",
       " 'delta air lines': 'Delta Air Lines'}"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary = dict(zip(company_list_lower, company_list))\n",
    "dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mmiyazaki/dataiku/Design/DATA_DIR/code-envs/python/python36/lib/python3.6/site-packages/ipykernel_launcher.py:1: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "temp = df[df.noun.str.lower().isin(company_list_lower)][df.product_id.str.lower() != df.noun.str.lower()]\n",
    "index_list = temp.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_replace = pd.DataFrame(df.loc[df.index.isin(index_list), [\"product_id\", \"noun\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_replace = df_replace.replace({\"noun\": dictionary})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df.index.isin(index_list), \"product_id\"] = df_replace.noun"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grouping companies and nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "df['noun_lemmatized'] = df.noun.str.lower().apply(lemmatizer.lemmatize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_grouped = df.groupby([\"product_id\", 'noun_lemmatized']).agg({'product_id':'size',\n",
    "                                                                \"polarity_nltk\":'mean',\n",
    "                                                                \"polarity_textblob\":'mean',\n",
    "                                                                \"review_id\":\"unique\"}).rename(columns={'product_id':'count',\n",
    "                                                                                                             'polarity_nltk':'mean_polarity_nltk',\n",
    "                                                                                                             'polarity_textblob':'mean_polarity_textblob'}).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_grouped.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"There are %d nouns extracted\" %(df_grouped.shape[0]))\n",
    "\n",
    "model_path= dataiku.get_custom_variables()['model_path']\n",
    "nlp = init_spacy(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "punctuality_vec = nlp('punctuality').vector\n",
    "food_vec = nlp('food').vector\n",
    "luggage_vec = nlp('luggage').vector\n",
    "staff_vec = nlp('staff').vector\n",
    "\n",
    "companies = df_grouped.product_id.unique()\n",
    "n_clusters = dataiku.get_custom_variables(typed=True)['NUM_CLUSTERS']\n",
    "if type(n_clusters) != int:\n",
    "    n_clusters = ast.literal_eval(n_clusters)\n",
    "df_vectors = pd.DataFrame()\n",
    "\n",
    "df_grouped['k_means_clusters'] = np.nan\n",
    "df_grouped['group'] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "for company in companies:\n",
    "    df_sub = df_grouped[df_grouped.product_id == company]\n",
    "    asp_group = []\n",
    "    asp_vectors = []\n",
    "    for aspect in df_sub.noun_lemmatized:\n",
    "        dist_dic = {}\n",
    "        token_vector = nlp(aspect).vector\n",
    "        asp_vectors.append(token_vector)\n",
    "        # df_grouped[df_grouped.noun_lemmatized==aspect, 'noun_vector'] = token_vector\n",
    "        dist_dic['punctuality'] = vector_dist(token_vector, punctuality_vec)\n",
    "        dist_dic['food'] = vector_dist(token_vector, food_vec)\n",
    "        dist_dic['luggage'] = vector_dist(token_vector, luggage_vec)\n",
    "        dist_dic['staff'] = vector_dist(token_vector, staff_vec)\n",
    "        # group = min([dist_punc, dist_food, dist_lugg, dist_staf])\n",
    "        max_key = max(dist_dic, key=dist_dic.get)\n",
    "        asp_group.append(max_key)\n",
    "\n",
    "    df_grouped.loc[df_grouped.product_id == company, \"group\"] = asp_group\n",
    "    df_grouped.loc[df_grouped.noun_lemmatized.str.lower() == df_grouped.product_id.str.lower(), \"group\"] = \"company\"\n",
    "\n",
    "    df_vectors_sub = pd.DataFrame(asp_vectors)\n",
    "    df_vectors_sub['product_id'] = company\n",
    "    df_vectors_sub['noun_lemmatized'] = df_sub['noun_lemmatized'].values\n",
    "    df_vectors_sub['count'] = df_sub['count'].values\n",
    "\n",
    "    # -------------------------------------------------------------------------------- NOTEBOOK-CELL: CODE\n",
    "    # you have to cluster for each company\n",
    "    kmeans = cluster.KMeans(n_clusters=n_clusters)\n",
    "    kmeans.fit(asp_vectors)\n",
    "    labels = kmeans.labels_\n",
    "    df_grouped.loc[df_grouped.product_id == company, \"k_means_clusters\"] = labels\n",
    "    df_vectors_sub['k_means_clusters'] = labels\n",
    "    df_vectors = pd.concat([df_vectors, df_vectors_sub])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_grouped['k_means_clusters'] = df_grouped.k_means_clusters.astype(int)\n",
    "df_grouped[\"tb_importance\"] = df_grouped[\"count\"] * df_grouped[\"mean_polarity_textblob\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute recipe outputs from inputs\n",
    "# TODO: Replace this part by your actual code that computes the output, as a Pandas dataframe\n",
    "# NB: DSS also supports other kinds of APIs for reading and writing data. Please see doc.\n",
    "\n",
    "# aspect_sentiment_categorised_df = df_grouped # For this sample code, simply copy input to output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write recipe outputs\n",
    "aspect_sentiment_categorised = dataiku.Dataset(\"tweepy_aspect_sentiment_categorised\")\n",
    "aspect_sentiment_categorised.write_with_schema(df_grouped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write recipe outputs\n",
    "word_vectors = dataiku.Dataset(\"tweepy_word_vectors\")\n",
    "word_vectors.write_with_schema(df_vectors)"
   ]
  }
 ],
 "metadata": {
  "associatedRecipe": "compute_aspect_sentiment_prep_1",
  "creator": "admin",
  "customFields": {},
  "kernelspec": {
   "display_name": "Python (env python36)",
   "language": "python",
   "name": "py-dku-venv-python36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "tags": [
   "recipe-editor"
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
