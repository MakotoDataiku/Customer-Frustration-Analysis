{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dataiku\n",
    "from dataiku import pandasutils as pdu\n",
    "import pandas as pd\n",
    "import tweepy\n",
    "from pandas.io.json import json_normalize\n",
    "import sys\n",
    "import jsonpickle\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "atoken = \"1211674579879391233-VwxwWZO0nnio8wwBVzncqhjfCTmz8S\"\n",
    "asecret = \"SUNhejwVlBrsx1B8I7yqXkn9NzjOKR6kzejkAHmJHdOTP\"\n",
    "ckey = \"nSLLEuipKq6iJetBCLj33mv1J\"\n",
    "csecret = \"s9XGj7t3XMvsmX2EHWeUHex2VvthapZFmmpFAj40VsQV2SvnVg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace the API_KEY and API_SECRET with your application's key and secret.\n",
    "auth = tweepy.AppAuthHandler(ckey, csecret)\n",
    "\n",
    "api = tweepy.API(auth, wait_on_rate_limit=True, wait_on_rate_limit_notify=True)\n",
    "\n",
    "if (not api):\n",
    "    print (\"Can't Authenticate\")\n",
    "    sys.exit(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "companies = dataiku.get_custom_variables(typed=True)[\"company\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Air France\n",
      "search query : Air France -RT\n",
      "Downloading max 100 tweets\n",
      "Downloaded 84 tweets\n",
      "Downloaded 180 tweets\n",
      "Downloaded 180 tweets for Air France, Saved to /Users/mmiyazaki/dataiku/Design/DATA_DIR/managed_folders/ANALYSISOFCUSTOMERFRUSTRATIONSINAIRLINEINDUSTRIES/2o8aWzsk/tweets.txt\n",
      "Lufthansa\n",
      "search query : Lufthansa -RT\n",
      "Downloading max 100 tweets\n",
      "Downloaded 100 tweets\n",
      "Downloaded 100 tweets for Lufthansa, Saved to /Users/mmiyazaki/dataiku/Design/DATA_DIR/managed_folders/ANALYSISOFCUSTOMERFRUSTRATIONSINAIRLINEINDUSTRIES/2o8aWzsk/tweets.txt\n",
      "Ryanair\n",
      "search query : Ryanair -RT\n",
      "Downloading max 100 tweets\n",
      "Downloaded 100 tweets\n",
      "Downloaded 100 tweets for Ryanair, Saved to /Users/mmiyazaki/dataiku/Design/DATA_DIR/managed_folders/ANALYSISOFCUSTOMERFRUSTRATIONSINAIRLINEINDUSTRIES/2o8aWzsk/tweets.txt\n",
      "easyJet\n",
      "search query : easyJet -RT\n",
      "Downloading max 100 tweets\n",
      "Downloaded 100 tweets\n",
      "Downloaded 100 tweets for easyJet, Saved to /Users/mmiyazaki/dataiku/Design/DATA_DIR/managed_folders/ANALYSISOFCUSTOMERFRUSTRATIONSINAIRLINEINDUSTRIES/2o8aWzsk/tweets.txt\n",
      "United Airlines\n",
      "search query : United Airlines -RT\n",
      "Downloading max 100 tweets\n",
      "Downloaded 100 tweets\n",
      "Downloaded 100 tweets for United Airlines, Saved to /Users/mmiyazaki/dataiku/Design/DATA_DIR/managed_folders/ANALYSISOFCUSTOMERFRUSTRATIONSINAIRLINEINDUSTRIES/2o8aWzsk/tweets.txt\n",
      "Delta Air Lines\n",
      "search query : Delta Air Lines -RT\n",
      "Downloading max 100 tweets\n",
      "Downloaded 100 tweets\n",
      "Downloaded 100 tweets for Delta Air Lines, Saved to /Users/mmiyazaki/dataiku/Design/DATA_DIR/managed_folders/ANALYSISOFCUSTOMERFRUSTRATIONSINAIRLINEINDUSTRIES/2o8aWzsk/tweets.txt\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame()\n",
    "\n",
    "for company in companies:\n",
    "    print(company)\n",
    "    searchQuery = company + \" -RT\"# this is what we're searching for\n",
    "    print(\"search query :\", searchQuery)\n",
    "    maxTweets = 100 # Some arbitrary large number\n",
    "    tweetsPerQry = 100  # this is the max the API permits\n",
    "    tweepy_REST_API = dataiku.Folder(\"tweepy_REST_API\")\n",
    "    folder_path = tweepy_REST_API.get_path()\n",
    "    fName = folder_path + '/tweets.txt' # We'll store the tweets in a text file.\n",
    "\n",
    "\n",
    "    # If results from a specific ID onwards are reqd, set since_id to that ID.\n",
    "    # else default to no lower limit, go as far back as API allows\n",
    "    sinceId = None\n",
    "\n",
    "    # If results only below a specific ID are, set max_id to that ID.\n",
    "    # else default to no upper limit, start from the most recent tweet matching the search query.\n",
    "    max_id = -1\n",
    "\n",
    "    tweetCount = 0\n",
    "\n",
    "\n",
    "    # -------------------------------------------------------------------------------- NOTEBOOK-CELL: CODE\n",
    "    print(\"Downloading max {0} tweets\".format(maxTweets))\n",
    "    with open(fName, 'w') as f:\n",
    "        while tweetCount < maxTweets:\n",
    "            try:\n",
    "                if (max_id <= 0):\n",
    "                    if (not sinceId):\n",
    "                        new_tweets = api.search(q=searchQuery, count=tweetsPerQry, lang=\"en\")\n",
    "                    else:\n",
    "                        new_tweets = api.search(q=searchQuery, count=tweetsPerQry,\n",
    "                                                since_id=sinceId, lang=\"en\")\n",
    "                else:\n",
    "                    if (not sinceId):\n",
    "                        new_tweets = api.search(q=searchQuery, count=tweetsPerQry,\n",
    "                                                max_id=str(max_id - 1), lang=\"en\")\n",
    "                    else:\n",
    "                        new_tweets = api.search(q=searchQuery, count=tweetsPerQry,\n",
    "                                                max_id=str(max_id - 1),\n",
    "                                                since_id=sinceId, lang=\"en\")\n",
    "                if not new_tweets:\n",
    "                    print(\"No more tweets found\")\n",
    "                    break\n",
    "                for tweet in new_tweets:\n",
    "                    f.write(jsonpickle.encode(tweet._json, unpicklable=False) + '\\n')\n",
    "                    \n",
    "                    if len(tweet._json['entities']['urls']) == 0:\n",
    "                        links = \"\"\n",
    "                    else: \n",
    "                        links = tweet._json['entities']['urls'][0][\"expanded_url\"]\n",
    "                        \n",
    "                    if tweet._json[\"place\"]==None:\n",
    "                        country = \"\"\n",
    "                    else:\n",
    "                        country = tweet._json[\"place\"]['country']\n",
    "                        \n",
    "                    if tweet._json[\"geo\"] != None:\n",
    "                        coordinates = str(tweet._json[\"geo\"]['coordinates'])\n",
    "                    else:\n",
    "                        coordinates = \"\"\n",
    "\n",
    "                    new_row = pd.DataFrame({'timestamp':tweet._json['created_at'],\n",
    "                                            'tweet_id':[tweet._json[\"id\"]],\n",
    "                                            'text':tweet._json[\"text\"],\n",
    "                                            'hashtags':[tweet._json[\"entities\"][\"hashtags\"]],\n",
    "                                            \"links\":links,\n",
    "                                            # 'user_mentions':tweet._json[\"entities\"][\"user_mentions\"][0][\"screen_name\"],\n",
    "                                            # 'user_mentions_id':tweet._json[\"entities\"][\"user_mentions\"][0][\"id\"],\n",
    "                                            # 'user_mentions_indices':[tweet._json[\"entities\"][\"user_mentions\"][0][\"indices\"]],\n",
    "                                            'in_reply_to_status_id':tweet._json[\"in_reply_to_status_id\"],\n",
    "                                            'in_reply_to_user_id':tweet._json[\"in_reply_to_user_id\"],\n",
    "                                            'in_reply_to_screen_name':tweet._json[\"in_reply_to_screen_name\"],\n",
    "                                            'user_id':tweet._json[\"user\"][\"id\"],\n",
    "                                            'user_name':tweet._json[\"user\"][\"name\"],\n",
    "                                            'user_screen_name':tweet._json[\"user\"][\"screen_name\"],\n",
    "                                            'user_location':tweet._json[\"user\"][\"location\"],\n",
    "                                            'followers_count':tweet._json[\"user\"][\"followers_count\"],\n",
    "                                            'friends_count':tweet._json[\"user\"][\"friends_count\"],\n",
    "                                            'user_creation':tweet._json[\"user\"][\"created_at\"],\n",
    "                                            'favourites_count':tweet._json[\"user\"][\"favourites_count\"],\n",
    "                                            'coordinates':coordinates,\n",
    "                                            # 'geo':tweet._json[\"geo\"],\n",
    "                                            'country':country,\n",
    "                                            'retweet_count':tweet._json[\"retweet_count\"],\n",
    "                                            'retweeted':tweet._json[\"retweeted\"],\n",
    "                                            'lang':tweet._json[\"lang\"]\n",
    "                                        })\n",
    "                    \"\"\"new_row = json_normalize(tweet._json)[[ # 'contributors',\n",
    "                                                           # \"coordinates\",\n",
    "                                                           \"created_at\",\n",
    "                                                           'entities.hashtags', 'entities.urls',\n",
    "                                                           # 'geo',\n",
    "                                                           'id',\n",
    "                                                           \"in_reply_to_screen_name\", \"in_reply_to_status_id\", \"in_reply_to_user_id\",\n",
    "                                                           \"lang\",\n",
    "                                                           # 'place.country', 'place.country_code', 'place.full_name', 'place.id','place.name', 'place.place_type', 'place.url',\n",
    "                                                           'retweet_count', 'retweeted','text','user.created_at',\n",
    "                                                           'user.followers_count', 'user.following','user.id', 'user.screen_name',\n",
    "                                                           'user.time_zone']]\"\"\"\n",
    "                    new_row[\"company\"] = company\n",
    "                    df = df.append(new_row, ignore_index = True)\n",
    "\n",
    "                tweetCount += len(new_tweets)\n",
    "                print(\"Downloaded {0} tweets\".format(tweetCount))\n",
    "                max_id = new_tweets[-1].id\n",
    "            except tweepy.TweepError as e:\n",
    "                # Just exit if any error\n",
    "                print(\"some error : \" + str(e))\n",
    "                break\n",
    "\n",
    "    print (\"Downloaded {0} tweets for {1}, Saved to {2}\".format(tweetCount, company, fName))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <button style=\"display:none\" \n",
       "            class=\"btn btn-default ipython-export-btn\" \n",
       "            id=\"btn-df-19d312b4-6755-4e52-9f50-b45ef6b79121\" \n",
       "            onclick=\"_export_df('19d312b4-6755-4e52-9f50-b45ef6b79121')\">\n",
       "                Export dataframe\n",
       "            </button>\n",
       "            \n",
       "            <script>\n",
       "                \n",
       "                function _check_export_df_possible(dfid,yes_fn,no_fn) {\n",
       "                    console.log('Checking dataframe exportability...')\n",
       "                    if(!IPython || !IPython.notebook || !IPython.notebook.kernel || !IPython.notebook.kernel) {\n",
       "                        console.log('Export is not possible (IPython kernel is not available)')\n",
       "                        if(no_fn) {\n",
       "                            no_fn();\n",
       "                        }\n",
       "                    } else {\n",
       "                        var pythonCode = 'from dataiku.notebook.export import IPythonExporter;IPythonExporter._check_export_stdout(\"'+dfid+'\")';\n",
       "                        IPython.notebook.kernel.execute(pythonCode,{iopub: {output: function(resp) {\n",
       "                            console.info(\"Exportability response\", resp);\n",
       "                            var size = /^([0-9]+)x([0-9]+)$/.exec(resp.content.data || resp.content.text)\n",
       "                            if(!size) {\n",
       "                                console.log('Export is not possible (dataframe is not in-memory anymore)')\n",
       "                                if(no_fn) {\n",
       "                                    no_fn();\n",
       "                                }\n",
       "                            } else {\n",
       "                                console.log('Export is possible')\n",
       "                                if(yes_fn) {\n",
       "                                    yes_fn(1*size[1],1*size[2]);\n",
       "                                }\n",
       "                            }\n",
       "                        }}});\n",
       "                    }\n",
       "                }\n",
       "            \n",
       "                function _export_df(dfid) {\n",
       "                    \n",
       "                    var btn = $('#btn-df-'+dfid);\n",
       "                    var btns = $('.ipython-export-btn');\n",
       "                    \n",
       "                    _check_export_df_possible(dfid,function() {\n",
       "                        \n",
       "                        window.parent.openExportModalFromIPython('Pandas dataframe',function(data) {\n",
       "                            btns.prop('disabled',true);\n",
       "                            btn.text('Exporting...');\n",
       "                            var command = 'from dataiku.notebook.export import IPythonExporter;IPythonExporter._run_export(\"'+dfid+'\",\"'+data.exportId+'\")';\n",
       "                            var callback = {iopub:{output: function(resp) {\n",
       "                                console.info(\"CB resp:\", resp);\n",
       "                                _check_export_df_possible(dfid,function(rows, cols) {\n",
       "                                    $('#btn-df-'+dfid)\n",
       "                                        .css('display','inline-block')\n",
       "                                        .text('Export this dataframe ('+rows+' rows, '+cols+' cols)')\n",
       "                                        .prop('disabled',false);\n",
       "                                },function() {\n",
       "                                    $('#btn-df-'+dfid).css('display','none');\n",
       "                                });\n",
       "                            }}};\n",
       "                            IPython.notebook.kernel.execute(command,callback,{silent:false}); // yes, silent now defaults to true. figures.\n",
       "                        });\n",
       "                    \n",
       "                    }, function(){\n",
       "                            alert('Unable to export : the Dataframe object is not loaded in memory');\n",
       "                            btn.css('display','none');\n",
       "                    });\n",
       "                    \n",
       "                }\n",
       "                \n",
       "                (function(dfid) {\n",
       "                \n",
       "                    var retryCount = 10;\n",
       "                \n",
       "                    function is_valid_websock(s) {\n",
       "                        return s && s.readyState==1;\n",
       "                    }\n",
       "                \n",
       "                    function check_conn() {\n",
       "                        \n",
       "                        if(!IPython || !IPython.notebook) {\n",
       "                            // Don't even try to go further\n",
       "                            return;\n",
       "                        }\n",
       "                        \n",
       "                        // Check if IPython is ready\n",
       "                        console.info(\"Checking conn ...\")\n",
       "                        if(IPython.notebook.kernel\n",
       "                        && IPython.notebook.kernel\n",
       "                        && is_valid_websock(IPython.notebook.kernel.ws)\n",
       "                        ) {\n",
       "                            \n",
       "                            _check_export_df_possible(dfid,function(rows, cols) {\n",
       "                                $('#btn-df-'+dfid).css('display','inline-block');\n",
       "                                $('#btn-df-'+dfid).text('Export this dataframe ('+rows+' rows, '+cols+' cols)');\n",
       "                            });\n",
       "                            \n",
       "                        } else {\n",
       "                            console.info(\"Conditions are not ok\", IPython.notebook.kernel);\n",
       "                            \n",
       "                            // Retry later\n",
       "                            \n",
       "                            if(retryCount>0) {\n",
       "                                setTimeout(check_conn,500);\n",
       "                                retryCount--;\n",
       "                            }\n",
       "                            \n",
       "                        }\n",
       "                    };\n",
       "                    \n",
       "                    setTimeout(check_conn,100);\n",
       "                    \n",
       "                })(\"19d312b4-6755-4e52-9f50-b45ef6b79121\");\n",
       "                \n",
       "            </script>\n",
       "            \n",
       "        <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>text</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>links</th>\n",
       "      <th>in_reply_to_status_id</th>\n",
       "      <th>in_reply_to_user_id</th>\n",
       "      <th>in_reply_to_screen_name</th>\n",
       "      <th>user_id</th>\n",
       "      <th>user_name</th>\n",
       "      <th>user_screen_name</th>\n",
       "      <th>user_location</th>\n",
       "      <th>followers_count</th>\n",
       "      <th>friends_count</th>\n",
       "      <th>user_creation</th>\n",
       "      <th>favourites_count</th>\n",
       "      <th>coordinates</th>\n",
       "      <th>country</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>retweeted</th>\n",
       "      <th>lang</th>\n",
       "      <th>company</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Mon May 04 17:36:45 +0000 2020</td>\n",
       "      <td>1257363549476847618</td>\n",
       "      <td>@zabulka @SimonCalder Soooo agree. S Korea, Cz...</td>\n",
       "      <td>[]</td>\n",
       "      <td>https://twitter.com/i/web/status/1257363549476...</td>\n",
       "      <td>1256964215727890432</td>\n",
       "      <td>55310845</td>\n",
       "      <td>zabulka</td>\n",
       "      <td>2227929828</td>\n",
       "      <td>Frances Gray</td>\n",
       "      <td>Franki2013Gray</td>\n",
       "      <td>Tadworth, Surrey</td>\n",
       "      <td>109</td>\n",
       "      <td>284</td>\n",
       "      <td>Tue Dec 03 08:06:45 +0000 2013</td>\n",
       "      <td>1882</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>en</td>\n",
       "      <td>Air France</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Mon May 04 17:33:33 +0000 2020</td>\n",
       "      <td>1257362742488567817</td>\n",
       "      <td>@Bardia84261966 The one one the right is at la...</td>\n",
       "      <td>[]</td>\n",
       "      <td>https://twitter.com/i/web/status/1257362742488...</td>\n",
       "      <td>1257352075089973249</td>\n",
       "      <td>1237435796820557824</td>\n",
       "      <td>Bardia84261966</td>\n",
       "      <td>1039530318921908225</td>\n",
       "      <td>Lazy 'Counterpart'</td>\n",
       "      <td>SuperLazy6</td>\n",
       "      <td></td>\n",
       "      <td>59</td>\n",
       "      <td>258</td>\n",
       "      <td>Tue Sep 11 15:05:16 +0000 2018</td>\n",
       "      <td>2545</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>en</td>\n",
       "      <td>Air France</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Mon May 04 17:33:03 +0000 2020</td>\n",
       "      <td>1257362620274933769</td>\n",
       "      <td>Ryanair boss will not be happy with Air France...</td>\n",
       "      <td>[]</td>\n",
       "      <td>https://youtu.be/Dh-2McZe2KU</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>1106193729235402753</td>\n",
       "      <td>üá¨üáß ùóïùóï üá¨üáß</td>\n",
       "      <td>BrexitBetrayed</td>\n",
       "      <td>United Kingdom üá¨üáß</td>\n",
       "      <td>20912</td>\n",
       "      <td>6256</td>\n",
       "      <td>Thu Mar 14 14:01:51 +0000 2019</td>\n",
       "      <td>24468</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>en</td>\n",
       "      <td>Air France</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Mon May 04 17:31:32 +0000 2020</td>\n",
       "      <td>1257362235464318980</td>\n",
       "      <td>Coronavirus aid: Air France 'must cut domestic...</td>\n",
       "      <td>[]</td>\n",
       "      <td>http://dlvr.it/RW0HNY</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>60180662</td>\n",
       "      <td>Emad Rassmy</td>\n",
       "      <td>erassmy</td>\n",
       "      <td>Palm Desert, California</td>\n",
       "      <td>357</td>\n",
       "      <td>171</td>\n",
       "      <td>Sat Jul 25 23:49:57 +0000 2009</td>\n",
       "      <td>12</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>en</td>\n",
       "      <td>Air France</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Mon May 04 17:31:10 +0000 2020</td>\n",
       "      <td>1257362143919431681</td>\n",
       "      <td>Ryanair boss will not be happy with Air France...</td>\n",
       "      <td>[]</td>\n",
       "      <td>https://youtu.be/Dh-2McZe2KU</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>857696952884097025</td>\n",
       "      <td>Hugh de Morville</td>\n",
       "      <td>MorvilleHugh</td>\n",
       "      <td></td>\n",
       "      <td>203</td>\n",
       "      <td>175</td>\n",
       "      <td>Thu Apr 27 20:44:23 +0000 2017</td>\n",
       "      <td>12981</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>en</td>\n",
       "      <td>Air France</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        timestamp             tweet_id                                               text hashtags                                              links in_reply_to_status_id  in_reply_to_user_id in_reply_to_screen_name              user_id           user_name user_screen_name            user_location  followers_count  friends_count                   user_creation  favourites_count coordinates country  retweet_count  retweeted lang     company\n",
       "0  Mon May 04 17:36:45 +0000 2020  1257363549476847618  @zabulka @SimonCalder Soooo agree. S Korea, Cz...       []  https://twitter.com/i/web/status/1257363549476...   1256964215727890432             55310845                 zabulka           2227929828        Frances Gray   Franki2013Gray         Tadworth, Surrey              109            284  Tue Dec 03 08:06:45 +0000 2013              1882                                  0      False   en  Air France\n",
       "1  Mon May 04 17:33:33 +0000 2020  1257362742488567817  @Bardia84261966 The one one the right is at la...       []  https://twitter.com/i/web/status/1257362742488...   1257352075089973249  1237435796820557824          Bardia84261966  1039530318921908225  Lazy 'Counterpart'       SuperLazy6                                        59            258  Tue Sep 11 15:05:16 +0000 2018              2545                                  0      False   en  Air France\n",
       "2  Mon May 04 17:33:03 +0000 2020  1257362620274933769  Ryanair boss will not be happy with Air France...       []                       https://youtu.be/Dh-2McZe2KU                  None                 None                    None  1106193729235402753            üá¨üáß ùóïùóï üá¨üáß   BrexitBetrayed        United Kingdom üá¨üáß            20912           6256  Thu Mar 14 14:01:51 +0000 2019             24468                                  1      False   en  Air France\n",
       "3  Mon May 04 17:31:32 +0000 2020  1257362235464318980  Coronavirus aid: Air France 'must cut domestic...       []                              http://dlvr.it/RW0HNY                  None                 None                    None             60180662         Emad Rassmy          erassmy  Palm Desert, California              357            171  Sat Jul 25 23:49:57 +0000 2009                12                                  0      False   en  Air France\n",
       "4  Mon May 04 17:31:10 +0000 2020  1257362143919431681  Ryanair boss will not be happy with Air France...       []                       https://youtu.be/Dh-2McZe2KU                  None                 None                    None   857696952884097025    Hugh de Morville     MorvilleHugh                                       203            175  Thu Apr 27 20:44:23 +0000 2017             12981                                  0      False   en  Air France"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "582 rows successfully written (WqsFZ0aYQs)\n"
     ]
    }
   ],
   "source": [
    "py_recipe_output = dataiku.Dataset(\"tweepy_REST_API\")\n",
    "py_recipe_output.write_with_schema(df)"
   ]
  }
 ],
 "metadata": {
  "associatedRecipe": "recipe from notebook tweepy RESTAPI test",
  "creator": "admin",
  "customFields": {},
  "kernelspec": {
   "display_name": "Python (env python36)",
   "language": "python",
   "name": "py-dku-venv-python36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "tags": [
   "recipe-editor"
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
